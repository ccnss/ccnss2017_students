{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrence, Depth and High-dimensional data\n",
    "# Keras ANN notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we introduce the MNIST dataset, and present basic methods to setup and train a *artificial neural network (ANN)* with the Keras package.\n",
    "\n",
    "The following elements will be presented:\n",
    "\n",
    "* MNIST dataset\n",
    "* data pre-processing: size reduction, scaling\n",
    "* shallow ann: setup and training \n",
    "* loss and accuracy graphs\n",
    "* optimizer options: learning rate\n",
    "* object-oriented interface\n",
    "* deep ann: setup and training\n",
    "* generating network architecture graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "* THE MNIST DATABASE of handwritten digits (http://yann.lecun.com/exdb/mnist/)\n",
    "* [Are we there yet?](http://rodrigob.github.io/are_we_there_yet/build/)\n",
    "* [Keras](https://keras.io/): The Python Deep Learning library\n",
    "* Getting started with the Keras [functional API](https://keras.io/getting-started/functional-api-guide/)\n",
    "* Getting started with the Keras Sequential model ([object oriented](https://keras.io/getting-started/sequential-model-guide/))\n",
    "* Keras [model visualization](https://keras.io/visualization/)\n",
    "* An [overview](http://sebastianruder.com/optimizing-gradient-descent/) of gradient descent optimization algorithms\n",
    "* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)\n",
    "* colah's [blog](http://colah.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please execute the cell bellow in order to initialize the notebook environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "# %matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import mod3\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (5.0, 4.0), 'lines.linewidth': 2.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset has the following properties:\n",
    "* fixed-size images of handwritten digits\n",
    "* images are size-normalized and centered\n",
    "* training set of 60,000 samples, test set of 10,000 samples\n",
    "* test samples are from different writters\n",
    "\n",
    "The MNIST dataset has enough complexity to apply key machine learning concepts while not being too computationaly intensive to train. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 1**\n",
    "\n",
    "The Keras framework provides access to several popular datasets, with the module `keras.datasets`. The MNIST dataset is loaded as follows:\n",
    "```\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "Load the MNIST dataset and review its basic properties\n",
    "\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* load the MNIST dataset with `mnist.load_data()`\n",
    "* print the shapes of `x_train`, `y_train`, `x_test` and `y_test`\n",
    "* plot a few random samples from the training set using `plt.imshow(img, cmap=plt.cm.gray)`\n",
    "* plot the distribution of pixel values in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import MNIST dataset\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train_orig, y_train), (x_test_orig, y_test) = mnist.load_data()\n",
    "\n",
    "# print dataset properties\n",
    "print('[Dataset properties]')\n",
    "print('train set data shape:', x_train_orig.shape)\n",
    "print('train set label shape:', y_train.shape)\n",
    "print('test set data shape:', x_test_orig.shape)\n",
    "print('test set label shape:', y_test.shape)\n",
    "print()\n",
    "\n",
    "# print sample properties\n",
    "print('[Sample properties]')\n",
    "print('label:', y_train[0])\n",
    "print('shape:', x_train_orig[0].shape)\n",
    "print('min:', x_train_orig[0].min())\n",
    "print('max:', x_train_orig[0].max())\n",
    "\n",
    "# select n_show samples randomly\n",
    "n_show = 9\n",
    "selected = np.random.randint(0, high=len(x_train_orig), size=n_show)\n",
    "\n",
    "# plot samples\n",
    "plt.figure(figsize=(9, 1))\n",
    "for idx, img in enumerate(x_train_orig[selected]):\n",
    "    plt.subplot(1, n_show, idx+1)\n",
    "    plt.imshow(img, cmap=plt.cm.gray)\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of pixel values in the train set\n",
    "plt.figure()\n",
    "plt.title('Distribution of pixel values')    \n",
    "plt.hist(x_train_orig.flatten(), bins=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "[Dataset properties]\n",
    "train set data shape: (60000, 28, 28)\n",
    "train set label shape: (60000,)\n",
    "test set data shape: (10000, 28, 28)\n",
    "test set label shape: (10000,)\n",
    "\n",
    "[Sample properties]\n",
    "label: 5\n",
    "shape: (28, 28)\n",
    "min: 0\n",
    "max: 255\n",
    "```\n",
    "<img src=\"fig/keras_ann_mnist.png\" style=\"width:90%;height:90%;display:inline;margin:1px\">\n",
    "<img src=\"fig/keras_ann_mnist_hist.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE 2**\n",
    "\n",
    "Several pre-processing steps should take place before training the MNIST dataset.\n",
    "\n",
    "* reduce pixel count by factor of 4, in order to train the dataset on CPU\n",
    "* scale pixel intensities between 0 and 1\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* drop one in every two pixels with smart indexing kung-fu \n",
    "* scale pixel intensities between 0 and 1\n",
    "* print the shapes of `x_train`, `y_train`, `x_test` and `y_test`\n",
    "* plot a few samples\n",
    "* plot the distribution of the pixel values in the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce pixel count by factor of 4\n",
    "x_train = x_train_orig[:, ::2, ::2].copy()\n",
    "x_test = x_test_orig[:, ::2, ::2].copy()\n",
    "\n",
    "# scale intensities between 0 and 1\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print('train set shape:', x_train.shape)\n",
    "print('test set shape:', x_train.shape)\n",
    "      \n",
    "# plot samples\n",
    "plt.figure(figsize=(9, 0.75))\n",
    "for idx, img in enumerate(x_train[selected]):\n",
    "    plt.subplot(1, n_show, idx+1)\n",
    "    plt.imshow(img, cmap=plt.cm.gray)\n",
    "    plt.gca().get_xaxis().set_visible(False)\n",
    "    plt.gca().get_yaxis().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot the distribution of pixel values in the train set\n",
    "plt.figure()\n",
    "plt.title('Distribution of pixel values')    \n",
    "plt.hist(x_train.flatten(), bins=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "train set shape: (60000, 14, 14)\n",
    "test set shape: (60000, 14, 14)\n",
    "```\n",
    "<img src=\"fig/keras_ann_mnist_small.png\" style=\"width:90%;height:90%;display:inline;margin:1px\">\n",
    "<img src=\"fig/keras_ann_mnist_hist_scaled.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow ANN\n",
    "\n",
    "<img src=\"fig/keras_ann_shallow_schema.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting dataset format for ANN encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the graph above, the inputs to the ANN are arranged as a vector of pixels, and the outputs are arranged as a vector of labels. The outputs are encoded as *1-out-of-n*, where all units are $0$ except for the unit corresponding to the class, i.e. label $2$ is encoded as $(0, 0, 1, 0, 0, 0, 0, 0, 0, 0)$.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* use function `keras.utils.to_categorical()` to transform the labels to *1-out-of-n* encoding\n",
    "* transform the samples to the required shape with smart indexing kung-fu (totaly unrelated to python package `kungfu`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numeric class to 1-of-n binary vector\n",
    "n_out = 10\n",
    "labels_train = keras.utils.to_categorical(y_train, n_out)\n",
    "labels_test = keras.utils.to_categorical(y_test, n_out)\n",
    "\n",
    "# transform image to vector\n",
    "input_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "input_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "input_train_shape = input_train.shape[1:]\n",
    "\n",
    "print('train set data shape:', input_train.shape)\n",
    "print('train set label shape:', labels_test.shape)\n",
    "print('test set data shape:', input_test.shape)\n",
    "print('test set label shape:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "train set data shape: (60000, 196)\n",
    "train set label shape: (10000, 10)\n",
    "test set data shape: (10000, 196)\n",
    "test set label shape: (10000, 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a shallow ANN uses the following elements:\n",
    "\n",
    "* `keras.layersInput()` sets the input\n",
    "* `keras.layersDense()` adds a fully connected layer\n",
    "* `keras.models.Model()` defines the ANN model\n",
    "* `compile()` method of `keras.models.Model` implements the ANN in Tensorflow\n",
    "* `summary()` method of `keras.models.Model` prints the ANN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input (InputLayer)           (None, 196)               0         \n",
    "_________________________________________________________________\n",
    "fc1 (Dense)                  (None, 256)               50432     \n",
    "_________________________________________________________________\n",
    "output (Dense)               (None, 10)                2570      \n",
    "=================================================================\n",
    "Total params: 53,002.0\n",
    "Trainable params: 53,002.0\n",
    "Non-trainable params: 0.0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN is trained by calling the method `fit()` and specifying the train set. The trained model is evaluated by calling the method `evaluate()` and the specifying test set.\n",
    "\n",
    "The default named parameters of the `fit()` method are the following:\n",
    "* `batch_size=32`\n",
    "* `epochs=1`\n",
    "* `verbose=1`\n",
    "* `shuffle=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(input_train, labels_train)\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Train parameters]')\n",
    "for item in history.params:\n",
    "    print(item+':', history.params[item])\n",
    "    \n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], format(np.mean(evaluation[0]), '.4f'))\n",
    "print('test', history.params['metrics'][1], format(np.mean(evaluation[1]), '.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "Epoch 1/1\n",
    "60000/60000 [==============================] - 4s - loss: 0.0981 - acc: 0.1559     \n",
    "\n",
    "[Train parameters]\n",
    "metrics: ['loss', 'acc']\n",
    "samples: 60000\n",
    "batch_size: 32\n",
    "epochs: 1\n",
    "do_validation: False\n",
    "verbose: 1\n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0895\n",
    "test acc 0.1804\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing default train parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the ANN under the following conditions:\n",
    "\n",
    "* 5 training epochs\n",
    "* batch size of 128\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* Call the `fit()` method with relevant names parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "\n",
    "history = model.fit(input_train, labels_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=n_batch_size)\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Train parameters]')\n",
    "for item in history.params:\n",
    "    print(item+':', history.params[item])\n",
    "    \n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], format(np.mean(evaluation[0]), '.4f'))\n",
    "print('test', history.params['metrics'][1], format(np.mean(evaluation[1]), '.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "Epoch 1/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0898 - acc: 0.1506     \n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0897 - acc: 0.1620     \n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0896 - acc: 0.1770     \n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0895 - acc: 0.1924     \n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0894 - acc: 0.2083     \n",
    "\n",
    "[Train parameters]\n",
    "metrics: ['loss', 'acc']\n",
    "samples: 60000\n",
    "batch_size: 128\n",
    "epochs: 5\n",
    "do_validation: False\n",
    "verbose: 1\n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0894\n",
    "test acc 0.2213\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting the ANN weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the ANN keeps updating its weights with each call to the `fit()` method.\n",
    "Resetting the network requires to redefine its structure and compiling.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* Redefine the network, compile and train\n",
    "* compare the performance of 1 epoch vs 5 epochs training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(input_train, labels_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=n_batch_size)\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], format(np.mean(evaluation[0]), '.4f'))\n",
    "print('test', history.params['metrics'][1], format(np.mean(evaluation[1]), '.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "Epoch 1/5\n",
    "60000/60000 [==============================] - 2s - loss: 0.1332 - acc: 0.0825     \n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 2s - loss: 0.0917 - acc: 0.1470     \n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 2s - loss: 0.0904 - acc: 0.1804     \n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 2s - loss: 0.0901 - acc: 0.1964     \n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 2s - loss: 0.0899 - acc: 0.2005     \n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0899\n",
    "test acc 0.2097\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the learning rate $eta$ requires creating an optimizer instance,\n",
    "and passing it to the relevant optimizer option:\n",
    "```\n",
    "from keras import optimizers\n",
    "sgd = optimizers.SGD(lr=2)\n",
    "```\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* create an optimizer instance, set $\\eta=2$, and pass it to the `compile()` method\n",
    "* retrain network for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(input_train, labels_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=n_batch_size,\n",
    "                    shuffle=True)\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], format(np.mean(evaluation[0]), '.4f'))\n",
    "print('test', history.params['metrics'][1], format(np.mean(evaluation[1]), '.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "Epoch 1/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0750 - acc: 0.5020     \n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 2s - loss: 0.0444 - acc: 0.7879     \n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0340 - acc: 0.8398     \n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 1s - loss: 0.0294 - acc: 0.8578     \n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 2s - loss: 0.0268 - acc: 0.8678     \n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0248\n",
    "test acc 0.8780\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and accuracy visualisation (Tensorboard)\n",
    "\n",
    "Open up a terminal and start a TensorBoard server that will read logs stored at `/tmp/ann`.\n",
    "\n",
    "`tensorboard --logdir=/tmp/ann`\n",
    "\n",
    "This allows us to monitor training in the TensorBoard web interface at http://127.0.0.1:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(input_train, labels_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=n_batch_size,\n",
    "                    validation_data=(input_test, labels_test),\n",
    "                    verbose=0,\n",
    "                    callbacks=[TensorBoard(log_dir='/tmp/ann')])\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], format(np.mean(evaluation[0]), '.4f'))\n",
    "print('test', history.params['metrics'][1], format(np.mean(evaluation[1]), '.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "\n",
    "<img src=\"fig/tensorboard_ann_1.png\" style=\"display:inline;margin:1px\"><img src=\"fig/tensorboard_ann_2.png\" style=\"display:inline;margin:1px\">\n",
    "<img src=\"fig/tensorboard_ann_3.png\" style=\"display:inline;margin:1px\"><img src=\"fig/tensorboard_ann_4.png\" style=\"display:inline;margin:1px\">\n",
    "\n",
    "```\n",
    "[Model evaluation]\n",
    "test loss 0.0249\n",
    "test acc 0.8789\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and accuracy visualisation (Animated plots)\n",
    "\n",
    "The evolution of loss and accuracy can also be monitored by querying the method `evaluate()` multiple times per epoch. However, the `fit()` method only trains entire epochs. One solution is to split the train set into multiple chunks, and instruct `fit()` to train each chunk as if it was the entire train set.\n",
    "\n",
    "First, let's create a looping structure splits the train set into multiple chunks.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* insert a loop in the code below that splits the list `indexes` into `n_eval` chunks of lenght `n_chunk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "n_eval = 5\n",
    "n_chunk = int(n/n_eval)\n",
    "\n",
    "indexes = np.arange(n)\n",
    "\n",
    "# insert looping kung-fu here\n",
    "for chunk in [indexes[i:i+n_chunk] for i in xrange(0, n, n_chunk)]:\n",
    "\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "[0 1 2 3 4 5 6 7 8 9]\n",
    "[10 11 12 13 14 15 16 17 18 19]\n",
    "[20 21 22 23 24 25 26 27 28 29]\n",
    "[30 31 32 33 34 35 36 37 38 39]\n",
    "[40 41 42 43 44 45 46 47 48 49]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup network\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "n = input_train.shape[0]\n",
    "n_eval = 5\n",
    "n_chunk = int(n/n_eval)\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "evaluate_train = model.evaluate(input_train, labels_train, verbose=0)\n",
    "evaluate_test = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "loss_train = [evaluate_train[0]]\n",
    "loss_test = [evaluate_test[0]]\n",
    "accuracy_train = [evaluate_train[1]]\n",
    "accuracy_test = [evaluate_test[1]]\n",
    "\n",
    "x_range = [0]\n",
    "\n",
    "fig=plt.figure(figsize=(9, 3))\n",
    "\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "\n",
    "fig1 = plt.subplot(gs[0])\n",
    "plt.plot(x_range, loss_train, 'C0', alpha=0.8, label='loss train')\n",
    "plt.plot(x_range, loss_test, 'C1', alpha=0.8, label='loss test')\n",
    "\n",
    "plt.ylim([0, max(loss_train[0], loss_test[0])*1.1])\n",
    "plt.xlim([0, n_epochs*1.1])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch (n)')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "fig2 = plt.subplot(gs[1])\n",
    "plt.plot(x_range, loss_train, 'C0', alpha=0.8, label='loss train')\n",
    "plt.plot(x_range, loss_test, 'C1', alpha=0.8, label='loss test')\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, n_epochs*1.1])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch (n)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "\n",
    "indexes = np.arange(n)\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    np.random.shuffle(indexes)\n",
    "\n",
    "    # insert looping kung-fu here\n",
    "    \n",
    "    for chunk in [indexes[i:i+n_chunk] for i in xrange(0, n, n_chunk)]:\n",
    "    \n",
    "        history = model.fit(input_train[chunk], labels_train[chunk],\n",
    "                            epochs=1,\n",
    "                            batch_size=n_batch_size,\n",
    "                            verbose=0)\n",
    "        \n",
    "        evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "        \n",
    "        loss_train += history.history['loss']\n",
    "        accuracy_train += history.history['acc']\n",
    "        loss_test += [evaluation[0]]\n",
    "        accuracy_test += [evaluation[1]]\n",
    "        \n",
    "        x_range += [x_range[-1]+len(chunk)/n_chunk/n_eval]\n",
    "\n",
    "        fig1.plot(x_range, loss_train, 'C0', alpha=0.8, label='loss train')\n",
    "        fig1.plot(x_range, loss_test, 'C1', alpha=0.8, label='loss test')\n",
    "        fig2.plot(x_range, accuracy_train, 'C0', alpha=0.8, label='accuracy train')\n",
    "        fig2.plot(x_range, accuracy_test, 'C1', alpha=0.8, label='accuracy test')\n",
    "        fig.canvas.draw()        \n",
    "\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], format(np.mean(evaluation[0]), '.4f'))\n",
    "print('test', history.params['metrics'][1], format(np.mean(evaluation[1]), '.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "<img src=\"fig/ann_keras_plot_custom.png\">\n",
    "\n",
    "```\n",
    "[Model evaluation]\n",
    "test loss 0.0247837326184\n",
    "test acc 0.8787\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object oriented interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an object oriented (OO) interface, as you can see in the exemple below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# setup network\n",
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "\n",
    "model = Sequential()\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=n_fc1, input_dim=input_train_shape[0]))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.add(Dense(units=n_out))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 256)               50432     \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 256)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 10)                2570      \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 10)                0         \n",
    "=================================================================\n",
    "Total params: 53,002.0\n",
    "Trainable params: 53,002.0\n",
    "Non-trainable params: 0.0\n",
    "_________________________________________________________________\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig/keras_ann_deep_schema.png\" style=\"width:50%;height:50%;display:inline;margin:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network setup and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding more layers to the network is done by stacking additional hidden layers.\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* Add two additional fully connected layers to the ANN\n",
    "* set layer size to 64\n",
    "* set batch size to 32 and $\\eta$ to 3\n",
    "* retrain with  batch size of 128 and observe the difference in performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup network\n",
    "n_out = 10\n",
    "n_fc1 = 64\n",
    "n_fc2 = n_fc1\n",
    "n_fc3 = n_fc1\n",
    "n_epochs = 5\n",
    "n_batch_size = 32\n",
    "eta = 3\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc1')(input_layer)\n",
    "x = Dense(n_fc2, activation='sigmoid', name='fc2')(x)\n",
    "x = Dense(n_fc3, activation='sigmoid', name='fc3')(x)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "sgd = optimizers.SGD(lr=eta)\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(input_train, labels_train,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=n_batch_size,\n",
    "                    shuffle=True)\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "\n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], format(np.mean(evaluation[0]), '.4f'))\n",
    "print('test', history.params['metrics'][1], format(np.mean(evaluation[1]), '.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT (with batch size = 32)**\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input (InputLayer)           (None, 196)               0         \n",
    "_________________________________________________________________\n",
    "fc1 (Dense)                  (None, 64)                12608     \n",
    "_________________________________________________________________\n",
    "fc2 (Dense)                  (None, 64)                4160      \n",
    "_________________________________________________________________\n",
    "fc3 (Dense)                  (None, 64)                4160      \n",
    "_________________________________________________________________\n",
    "output (Dense)               (None, 10)                650       \n",
    "=================================================================\n",
    "Total params: 21,578.0\n",
    "Trainable params: 21,578.0\n",
    "Non-trainable params: 0.0\n",
    "_________________________________________________________________\n",
    "Epoch 1/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0899 - acc: 0.1251     \n",
    "Epoch 2/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0700 - acc: 0.4602     \n",
    "Epoch 3/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0377 - acc: 0.7696     \n",
    "Epoch 4/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0246 - acc: 0.8489     \n",
    "Epoch 5/5\n",
    "60000/60000 [==============================] - 3s - loss: 0.0196 - acc: 0.8782     \n",
    "\n",
    "[Model evaluation]\n",
    "test loss 0.0178\n",
    "test acc 0.8901\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will you MNIST me?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a mini-competition for training MNIST, where any ANN model is accepted under the following rules: \n",
    "\n",
    "**CCNSS MNIST competition manifesto **\n",
    "\n",
    "We abide to the the rules of training MNIST under the following conditions:\n",
    "* optimizer is restricted to SGD with momentum\n",
    "* train set is reduced to the fist 20,000 samples of MNIST\n",
    "* results are reported by averaging over 5 training sessions\n",
    "* communicate the changes made to the plain vanilla ANN provided below\n",
    "\n",
    "**INSTRUCTIONS**\n",
    "* improve on the plain vanilla ANN provided below\n",
    "\n",
    "```\n",
    "sgd = optimizers.SGD(lr=eta, decay=1e-6, momentum=0.9)\n",
    "\n",
    "input_train_comptetition = input_train[:20000]\n",
    "labels_train_competition = labels_train[:20000]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out = 10\n",
    "n_fc1 = 256\n",
    "n_epochs = 5\n",
    "n_batch_size = 128\n",
    "eta = 2\n",
    "\n",
    "input_train_competition = input_train[:20000]\n",
    "labels_train_competition = labels_train[:20000]\n",
    "\n",
    "input_layer = Input(shape=(input_train_shape), name='input')\n",
    "x = Dense(n_fc1, activation='sigmoid', name='fc1')(input_layer)\n",
    "output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "optimizers.SGD(lr=eta, decay=1e-6, momentum=0.9)\n",
    "\n",
    "model = Model(input_layer, output_layer)\n",
    "model.compile(loss='mean_squared_error', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(input_train_competition , labels_train_competition,\n",
    "                    epochs=n_epochs,\n",
    "                    batch_size=n_batch_size)\n",
    "\n",
    "evaluation = model.evaluate(input_test, labels_test, verbose=0)\n",
    "  \n",
    "print('\\n[Model evaluation]')\n",
    "print('test', history.params['metrics'][0], '\\t', format(np.mean(evaluation[0]), '.4f'))\n",
    "print('test', history.params['metrics'][1], '\\t', format(np.mean(evaluation[1]), '.4f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT (with batch size = 32)**\n",
    "```\n",
    "Epoch 1/5\n",
    "20000/20000 [==============================] - 0s - loss: 0.0860 - acc: 0.3224     \n",
    "Epoch 2/5\n",
    "20000/20000 [==============================] - 0s - loss: 0.0655 - acc: 0.6290     \n",
    "Epoch 3/5\n",
    "20000/20000 [==============================] - 0s - loss: 0.0495 - acc: 0.7404     \n",
    "Epoch 4/5\n",
    "20000/20000 [==============================] - 0s - loss: 0.0412 - acc: 0.7948     \n",
    "Epoch 5/5\n",
    "20000/20000 [==============================] - 0s - loss: 0.0363 - acc: 0.8226     \n",
    "\n",
    "[Model evaluation]\n",
    "test loss \t 0.0343\n",
    "test acc \t 0.8261\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating network architecture graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from IPython.display import display, SVG\n",
    "# from keras.utils import plot_model\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# # setup network\n",
    "# n_out = 10\n",
    "# n_fc1 = 256\n",
    "\n",
    "# input_layer = Input(shape=(input_train_shape), name='input')\n",
    "# x = Dense(n_fc1, activation='sigmoid', name='fc1')(input_layer)\n",
    "# output_layer = Dense(n_out, activation='sigmoid', name='output')(x)\n",
    "\n",
    "# model = Model(input_layer, output_layer)\n",
    "# model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "# # save model graph\n",
    "# plot_model(model, to_file='fig/ann_graph.png')\n",
    "\n",
    "# # plot model graph\n",
    "# display(SVG(model_to_dot(model).create(prog='dot', format='svg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXPECTED OUTPUT**\n",
    "<img src=\"fig/ann_graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENDED EXERCISE 1**\n",
    "\n",
    "Synthetic image expansion is a way to reduce overfitting by performing random transformations of samples during training, effectively never presenting the same image twice during training.\n",
    "\n",
    "Investigate the benefits of this technique, and compare to other techniques such as elastic distortions.\n",
    "\n",
    "**References:**\n",
    "* Simard, Patrice Y., David Steinkraus, and John C. Platt. [Best practices for convolutional neural networks applied to visual document analysis.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.10.5032&rep=rep1&type=pdf), in ICDAR, vol. 3, pp. 958-962. 2003.\n",
    "* The function `keras.preprocessing.image.ImageDataGenerator()` provides random affine transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENDED EXERCISE 2**\n",
    "\n",
    "The amount of parameters in deep ANNs rapidly increases with layer depth, and easily becomes much larger than the available train samples. This  introduces an important risk of overfitting, since deep ANNs have the capacity to fully memorize train sets of random data.\n",
    "\n",
    "Investigate the effectiveness of modern techniques to overcome this issue as the number of layers increases. Consider methods such as careful weight initialization, weight regularization, layer normalization, specialized activation functions, and stochastic gradient descent engine.\n",
    "\n",
    "**References:**\n",
    "* Zhang, Chiyuan, et al. [Understanding deep learning requires rethinking generalization](https://arxiv.org/pdf/1611.03530.pdf), arXiv preprint arXiv:1611.03530 (2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTENDED EXERCISE 3**\n",
    "\n",
    "*Dropout* is a popular regularization technique, in which random subsets of units are not available during each mini-batch. This is equivalent to training a large number of smaller networks in parallel, and pooling their average predictions at test time. In practice it reduces co-adaptation of units during training, and regularizes weights.\n",
    "\n",
    "Investigate the effectiveness of dropout, and compare it to other alternatives, such as batch norm, ELUs, etc.\n",
    "\n",
    "**References:**\n",
    "* S. Nitish, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. [Dropout: A simple way to prevent neural networks from overfitting](http://www.jmlr.org/papers/volume15/srivastava14a.old/source/srivastava14a.pdf). The Journal of Machine Learning Research (2014).\n",
    "* Sergey Ioffe, Christian Szegedy. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167).  arXiv preprint arXiv:1502.03167 (2015).\n",
    "* Clevert, Djork-Arn√©, Thomas Unterthiner, and Sepp Hochreiter. [Fast and accurate deep network learning by exponential linear units (elus)](http://arxiv.org/abs/1511.07289). arXiv preprint arXiv:1511.07289 (2015)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
